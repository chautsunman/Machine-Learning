{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.contrib.learn.datasets.load_dataset(\"mnist\")\n",
    "train_data = mnist.train.images\n",
    "train_labels = np.asarray(mnist.train.labels, dtype=np.int32)\n",
    "test_data = mnist.test.images\n",
    "test_labels = np.asarray(mnist.test.labels, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.reshape((-1, 28, 28, 1))\n",
    "train_labels = np.array([[0] * train_label + [1] + [0] * (9 - train_label) for train_label in train_labels])\n",
    "test_data = test_data.reshape((-1, 28, 28, 1))\n",
    "test_labels = np.array([[0] * test_label + [1] + [0] * (9 - test_label) for test_label in test_labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer base class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, activation=None, input_shape=None):\n",
    "        self.activation = activation\n",
    "        self.input_shape = input_shape\n",
    "    \n",
    "    def init(self, input_shape):\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = input_shape\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return x\n",
    "    \n",
    "    def get_derivative(self, errors, xs, ys):\n",
    "        return error\n",
    "    \n",
    "    def update_weights(self, errors, xs, ys, learning_rate):\n",
    "        return\n",
    "    \n",
    "    def relu(self, x):\n",
    "        return np.where(x > 0, x, 0)\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        exp = np.exp(x - np.max(x))\n",
    "        exp_sum = np.sum(exp)\n",
    "        return exp / exp_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    def __init__(self, units, activation=\"relu\", input_shape=None):\n",
    "        super().__init__(activation=activation, input_shape=input_shape)\n",
    "        self.units = units\n",
    "    \n",
    "    def init(self, input_shape):\n",
    "        super().init(input_shape)\n",
    "        self.weights = np.random.randn(input_shape[0], self.units) * np.sqrt(2 / input_shape[0])\n",
    "        self.biases = np.zeros(self.units)\n",
    "        self.output_shape = (self.units, )\n",
    "    \n",
    "    def predict(self, x):\n",
    "        y = np.dot(x, self.weights) + self.biases\n",
    "        \n",
    "        if self.activation == \"relu\":\n",
    "            y = self.relu(y)\n",
    "        elif self.activation == \"softmax\":\n",
    "            y = self.softmax(y)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    def get_derivative(self, errors, xs, ys):\n",
    "        if self.activation == \"relu\":\n",
    "            errors = np.where(ys > 0, errors, 0)\n",
    "        elif self.activation == \"softmax\":\n",
    "            errors = errors * ys * (1 - ys)\n",
    "        \n",
    "        return np.dot(errors, self.weights.transpose())\n",
    "    \n",
    "    def update_weights(self, errors, xs, ys, learning_rate):\n",
    "        if self.activation == \"relu\":\n",
    "            errors = np.where(ys > 0, errors, 0)\n",
    "        elif self.activation == \"softmax\":\n",
    "            errors = errors * ys * (1 - ys)\n",
    "        \n",
    "        for error, x in zip(errors, xs):\n",
    "            self.weights -= learning_rate * error * np.broadcast_to(x, (self.units, self.input_shape[0])).transpose()\n",
    "            self.biases -= learning_rate * error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2D(Layer):\n",
    "    def __init__(self, filters, kernel_size, activation=\"relu\", input_shape=None):\n",
    "        super().__init__(activation=activation, input_shape=input_shape)\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "    \n",
    "    def init(self, input_shape):\n",
    "        super().init(input_shape)\n",
    "        self.weights = np.zeros((self.kernel_size[0], self.kernel_size[1], input_shape[2], self.filters))\n",
    "        self.biases = np.zeros(self.filters)\n",
    "        self.output_shape = (input_shape[0], input_shape[1], self.filters)\n",
    "        self.padding_number = ((self.kernel_size[0] - 1) // 2, (self.kernel_size[1] - 1) // 2)\n",
    "    \n",
    "    def zero_pad(self, image):\n",
    "        image_padded = np.empty((image.shape[0] + self.padding_number[0] * 2, image.shape[1] + self.padding_number[1] * 2, image.shape[2]))\n",
    "        for channel_idx in range(image.shape[2]):\n",
    "            image_channel_padded = image[:, :, channel_idx]\n",
    "            image_channel_padded = np.insert(image_channel_padded, [0] * self.padding_number[0] + [image.shape[1]] * self.padding_number[0], 0, axis=0)\n",
    "            image_channel_padded = np.insert(image_channel_padded, [0] * self.padding_number[1] + [image.shape[1]] * self.padding_number[1], 0, axis=1)\n",
    "            image_padded[:, :, channel_idx] = image_channel_padded\n",
    "        return image_padded\n",
    "    \n",
    "    def convolve(self, image, kernel):\n",
    "        convoluted = np.zeros((self.output_shape[0], self.output_shape[1]))\n",
    "        for row_idx in range(self.output_shape[0]):\n",
    "            row_start = row_idx\n",
    "            row_end = row_start + self.kernel_size[0]\n",
    "            for col_idx in range(self.output_shape[1]):\n",
    "                col_start = col_idx\n",
    "                col_end = col_start + self.kernel_size[1]\n",
    "                receptive_field = image[row_start:row_end, col_start:col_end]\n",
    "                convoluted[row_idx, col_idx] = np.sum(receptive_field * kernel)\n",
    "        return convoluted\n",
    "    \n",
    "    def predict(self, x):\n",
    "        x_padded = self.zero_pad(x)\n",
    "        \n",
    "        y = np.zeros(self.output_shape)\n",
    "        \n",
    "        for filter_idx in range(self.filters):\n",
    "            feature_map = np.zeros((self.output_shape[0], self.output_shape[1]))\n",
    "            for channel_idx in range(x_padded.shape[2]):\n",
    "                feature_map += self.convolve(x_padded[:, :, channel_idx], self.weights[:, :, channel_idx, filter_idx])\n",
    "            feature_map += self.biases[filter_idx]\n",
    "            y[:, :, filter_idx] = feature_map\n",
    "        \n",
    "        if self.activation == \"relu\":\n",
    "            y = self.relu(y)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max-pooling layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPooling2D(Layer):\n",
    "    def __init__(self, pool_size, strides, input_shape=None):\n",
    "        super().__init__(input_shape=input_shape)\n",
    "        self.pool_size = pool_size\n",
    "        self.strides = strides\n",
    "    \n",
    "    def init(self, input_shape):\n",
    "        super().init(input_shape)\n",
    "        self.output_shape = ((input_shape[0] - self.pool_size[0]) // self.strides[0] + 1, (input_shape[1] - self.pool_size[1]) // self.strides[1] + 1, input_shape[2])\n",
    "    \n",
    "    def predict(self, x):\n",
    "        y = np.zeros(self.output_shape)\n",
    "        \n",
    "        for row_idx in range(self.output_shape[0]):\n",
    "            row_start = row_idx * self.pool_size[0]\n",
    "            row_end = row_start + self.pool_size[0]\n",
    "            for col_idx in range(self.output_shape[1]):\n",
    "                col_start = col_idx * self.pool_size[1]\n",
    "                col_end = col_start + self.pool_size[1]\n",
    "                for channel_idx in range(self.output_shape[2]):\n",
    "                    y[row_idx, col_idx, channel_idx] = np.max(x[row_start:row_end, col_start:col_end, channel_idx])\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flatten layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(Layer):\n",
    "    def __init__(self, input_shape=None):\n",
    "        super().__init__(input_shape=input_shape)\n",
    "    \n",
    "    def init(self, input_shape):\n",
    "        super().init(input_shape)\n",
    "        self.output_shape = (np.prod(np.array(input_shape)), )\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return x.reshape(self.output_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.input_shape = ()\n",
    "    \n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "        if len(self.layers) == 1:\n",
    "            self.input_shape = layer.input_shape\n",
    "    \n",
    "    def compile(self, optimizer=\"sgd\", loss=\"categorical_crossentropy\", learning_rate=0.01):\n",
    "        self.optimizer = optimizer\n",
    "        self.loss = loss\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        input_shape = self.input_shape\n",
    "        for i in range(len(self.layers)):\n",
    "            self.layers[i].init(input_shape)\n",
    "            input_shape = self.layers[i].output_shape\n",
    "    \n",
    "    def predict(self, xs):\n",
    "        ys = np.empty((xs.shape[0], self.layers[-1].output_shape[0]))\n",
    "        \n",
    "        for i, x in enumerate(xs):\n",
    "            y = x\n",
    "            for layer in self.layers:\n",
    "                y = layer.predict(y)\n",
    "            ys[i, :] = y\n",
    "        \n",
    "        return ys\n",
    "    \n",
    "    def error(self, xs, labels):\n",
    "        ys = self.predict(xs)\n",
    "        \n",
    "        if self.loss == \"categorical_crossentropy\":\n",
    "            return -np.sum(labels * np.log(ys))\n",
    "        \n",
    "        return 0\n",
    "    \n",
    "    def accuracy(self, xs, labels):\n",
    "        ys = self.predict(xs)\n",
    "        \n",
    "        return ys[np.argmax(ys, axis=1) == np.argmax(labels, axis=1)].shape[0] / labels.shape[0]\n",
    "    \n",
    "    def fit(self, xs, labels, epochs=1, batch_size=32):\n",
    "        for epoch in range(epochs):\n",
    "            for batch_idx in range(int(np.ceil(xs.shape[0] / batch_size))):\n",
    "                batch_start_idx = batch_idx * batch_size\n",
    "                batch_end_idx = batch_start_idx + batch_size\n",
    "                if batch_end_idx > xs.shape[0]:\n",
    "                    batch_end_idx = xs.shape[0]\n",
    "                x_batch = xs[batch_start_idx:batch_end_idx]\n",
    "                label_batch = labels[batch_start_idx:batch_end_idx]\n",
    "                \n",
    "                ys = [x_batch]\n",
    "                errors = []\n",
    "                for layer in self.layers:\n",
    "                    ys.append(np.zeros((batch_end_idx - batch_start_idx, ) + layer.output_shape))\n",
    "                    errors.append(np.zeros((batch_end_idx - batch_start_idx, ) + layer.output_shape))\n",
    "                \n",
    "                for data_idx in range(x_batch.shape[0]):\n",
    "                    for layer_idx, layer in enumerate(self.layers):\n",
    "                        ys[layer_idx + 1][data_idx] = layer.predict(ys[layer_idx][data_idx])\n",
    "                    \n",
    "                    if self.loss == \"categorical_crossentropy\":\n",
    "                        errors[-1][data_idx, np.argmax(label_batch[data_idx])] = -(1 / ys[-1][data_idx, np.argmax(label_batch[data_idx])])\n",
    "                \n",
    "                errors[-1] /= batch_end_idx - batch_start_idx\n",
    "                \n",
    "                for layer_idx in range(len(self.layers) - 1, -1, -1):\n",
    "                    self.layers[layer_idx].update_weights(errors[layer_idx], ys[layer_idx], ys[layer_idx + 1], self.learning_rate)\n",
    "                    \n",
    "                    if layer_idx != 0:\n",
    "                        errors[layer_idx - 1] = self.layers[layer_idx].get_derivative(errors[layer_idx], ys[layer_idx], ys[layer_idx + 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = Sequential()\n",
    "linear_model.add(Dense(10, input_shape=(784, ), activation=\"softmax\"))\n",
    "linear_model.compile(optimizer=\"sgd\", loss=\"categorical_crossentropy\", learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model.fit(train_data.reshape(-1, 784), train_labels, epochs=1, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"error:\", linear_model.error(test_data.reshape(-1, 784), test_labels))\n",
    "print(\"accuracy:\", linear_model.accuracy(test_data.reshape(-1, 784), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (5, 5), input_shape=(28, 28, 1), activation=\"relu\"))\n",
    "model.add(MaxPooling2D((2, 2), (2, 2)))\n",
    "model.add(Conv2D(64, (5, 5), activation=\"relu\"))\n",
    "model.add(MaxPooling2D((2, 2), (2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024, activation=\"relu\"))\n",
    "model.add(Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"sgd\", loss=\"categorical_crossentropy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
